{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbf6230",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/giulatona/iecon2025_tutorial/blob/main/notebooks/03_forecasting_models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069a6c8",
   "metadata": {},
   "source": [
    "# Deep Learning for Energy Forecasting - Model Training\n",
    "\n",
    "This notebook demonstrates training various forecasting models on the household power consumption dataset, from simple baselines to advanced deep learning architectures.\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement baseline forecasting models (naive seasonal)\n",
    "- Build and train feedforward neural networks for time series\n",
    "- Develop LSTM Encoder-Decoder architectures\n",
    "- Compare model performance and understand trade-offs\n",
    "- Apply proper evaluation metrics for time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42919260",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d386cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: []\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning frameworks\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b0ed4",
   "metadata": {},
   "source": [
    "### Data Download and Initial Loading\n",
    "\n",
    "We'll use the same household power consumption dataset from the UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a59a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally...\n",
      "Dataset found locally.\n",
      "Using data file: data/household_power_consumption.txt\n"
     ]
    }
   ],
   "source": [
    "# Download data if not available locally\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Check if running in Google Colab\n",
    "in_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if in_colab:\n",
    "    print(\"Running in Google Colab - downloading dataset...\")\n",
    "    !wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
    "    !unzip -q household_power_consumption.zip\n",
    "    data_file = 'household_power_consumption.txt'\n",
    "else:\n",
    "    print(\"Running locally...\")\n",
    "    data_file = 'data/household_power_consumption.txt'\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    # Download dataset if it doesn't exist locally\n",
    "    if not os.path.exists(data_file):\n",
    "        print(\"Dataset not found locally. Downloading...\")\n",
    "        url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip'\n",
    "        zip_file = 'data/household_power_consumption.zip'\n",
    "        \n",
    "        # Download the zip file\n",
    "        urllib.request.urlretrieve(url, zip_file)\n",
    "        print(\"Download completed. Extracting...\")\n",
    "        \n",
    "        # Extract the zip file\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall('data/')\n",
    "        \n",
    "        print(\"Extraction completed.\")\n",
    "    else:\n",
    "        print(\"Dataset found locally.\")\n",
    "\n",
    "print(f\"Using data file: {data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a188ab3",
   "metadata": {},
   "source": [
    "## 2. Import Preprocessing Functions\n",
    "\n",
    "We need the preprocessing functions from notebook 02. Here are the options for including them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33ee965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported preprocessing functions from local utilities module\n"
     ]
    }
   ],
   "source": [
    "# Import preprocessing functions from shared utilities module\n",
    "\n",
    "# Add the src directory to Python path for imports\n",
    "if in_colab:\n",
    "    # For Colab, install the package directly from GitHub\n",
    "    print(\"Installing iecon2025_tutorial package from GitHub...\")\n",
    "    try:\n",
    "        %pip install git+https://github.com/giulatona/iecon2025_tutorial.git\n",
    "        from iecon2025_tutorial.preprocessing import load_and_preprocess_data\n",
    "        print(\"✅ Successfully installed and imported preprocessing functions from GitHub\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Installation error: {e}\")\n",
    "        print(\"Please check the GitHub repository URL and try again\")\n",
    "else:\n",
    "    try:\n",
    "        from iecon2025_tutorial.preprocessing import load_and_preprocess_data\n",
    "        print(\"✅ Successfully imported preprocessing functions from local utilities module\")\n",
    "    except ImportError as e:\n",
    "        print(f\"❌ Import error: {e}\")\n",
    "        print(\"Please ensure the src/iecon2025_tutorial directory contains preprocessing.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72933700",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c848db4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  window_size: 24\n",
      "  forecast_horizon: 12\n",
      "  batch_size: 32\n",
      "  epochs: 50\n",
      "  validation_split: 0.2\n",
      "  early_stopping_patience: 10\n",
      "  reduce_lr_patience: 5\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'window_size': 24,          # 24 hours of historical data\n",
    "    'forecast_horizon': 12,     # Predict next 12 hours\n",
    "    'batch_size': 32,\n",
    "    'epochs': 50,\n",
    "    'validation_split': 0.2,\n",
    "    'early_stopping_patience': 10,\n",
    "    'reduce_lr_patience': 5\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1defcbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing pipeline...\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset using the comprehensive pipeline\n",
    "try:\n",
    "    # Use the comprehensive preprocessing pipeline\n",
    "    results = load_and_preprocess_data(\n",
    "        data_path=str(data_file),\n",
    "        downsample_freq='1H',  # Hourly data for faster training\n",
    "        window_size=TRAINING_CONFIG['window_size'],\n",
    "        forecast_horizon=TRAINING_CONFIG['forecast_horizon'],\n",
    "        batch_size=TRAINING_CONFIG['batch_size'],\n",
    "        target_columns=[0],  # Predict Global_active_power only\n",
    "        add_time_features=True,\n",
    "        add_holiday_features=False,  # Skip holidays for speed\n",
    "        verbose=True  # Enable function's verbose output\n",
    "    )\n",
    "    \n",
    "    # Extract the datasets and metadata\n",
    "    train_dataset = results['datasets']['train']\n",
    "    val_dataset = results['datasets']['val']\n",
    "    test_dataset = results['datasets']['test']\n",
    "    \n",
    "    scaler_params = results['scaler_params']\n",
    "    feature_names = results['feature_names']\n",
    "    preprocessing_info = results['preprocessing_info']\n",
    "    \n",
    "    # Get data shapes for model building\n",
    "    for inputs, targets in train_dataset.take(1):\n",
    "        input_shape = inputs.shape\n",
    "        target_shape = targets.shape\n",
    "        num_features = input_shape[-1]\n",
    "        break\n",
    "    \n",
    "    # Show final summary\n",
    "    print(f\"Ready for training - Input: {input_shape}, Target: {target_shape}\")\n",
    "    print(f\"Features: {preprocessing_info['feature_count']}, Samples: {preprocessing_info['split_info']['train_samples']:,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in preprocessing: {e}\")\n",
    "    print(\"Please check that preprocessing functions are available\")\n",
    "    results = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iecon2025-tutorial-z-J6dBct-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
