{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025eba7c",
   "metadata": {},
   "source": [
    "# Preprocessing, Windowing and Calendar Features\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/giulatona/iecon2025_tutorial/blob/main/notebooks/02_preprocessing_windowing_features.ipynb)\n",
    "\n",
    "This notebook demonstrates preprocessing techniques for time series forecasting:\n",
    "\n",
    "- **Data Preprocessing**: Cleaning, normalization, and scaling\n",
    "- **Windowing Techniques**: Creating sliding windows for sequence modeling\n",
    "- **Calendar Features**: Adding temporal features (to be used as exogenous features)\n",
    "\n",
    "**Dataset**: UCI Individual Household Electric Power Consumption  \n",
    "**Goal**: Prepare data for machine learning forecasting models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9a4b2",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2fddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f93005",
   "metadata": {},
   "source": [
    "### Dataset Download (Hidden)\n",
    "The following cell downloads the dataset if not available locally. This cell is hidden by default to keep the notebook clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64058aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally...\n",
      "Dataset not found locally. Downloading...\n",
      "Download completed. Extracting...\n",
      "Download completed. Extracting...\n",
      "Extraction completed.\n",
      "Using data file: data/household_power_consumption.txt\n",
      "Extraction completed.\n",
      "Using data file: data/household_power_consumption.txt\n"
     ]
    }
   ],
   "source": [
    "# Download data if not available locally\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Check if running in Google Colab\n",
    "in_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if in_colab:\n",
    "    print(\"Running in Google Colab - downloading dataset...\")\n",
    "    !wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
    "    !unzip -q household_power_consumption.zip\n",
    "    data_file = 'household_power_consumption.txt'\n",
    "else:\n",
    "    print(\"Running locally...\")\n",
    "    data_file = 'data/household_power_consumption.txt'\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    # Download dataset if it doesn't exist locally\n",
    "    if not os.path.exists(data_file):\n",
    "        print(\"Dataset not found locally. Downloading...\")\n",
    "        url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip'\n",
    "        zip_file = 'data/household_power_consumption.zip'\n",
    "        \n",
    "        # Download the zip file\n",
    "        urllib.request.urlretrieve(url, zip_file)\n",
    "        print(\"Download completed. Extracting...\")\n",
    "        \n",
    "        # Extract the zip file\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall('data/')\n",
    "        \n",
    "        print(\"Extraction completed.\")\n",
    "    else:\n",
    "        print(\"Dataset found locally.\")\n",
    "\n",
    "print(f\"Using data file: {data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f8b791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (2075259, 9)\n",
      "Date range: 16/12/2006 to 26/11/2010\n",
      "Dataset shape: (2075259, 9)\n",
      "Date range: 16/12/2006 to 26/11/2010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:24:00</td>\n",
       "      <td>4.216</td>\n",
       "      <td>0.418</td>\n",
       "      <td>234.840</td>\n",
       "      <td>18.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:25:00</td>\n",
       "      <td>5.360</td>\n",
       "      <td>0.436</td>\n",
       "      <td>233.630</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:26:00</td>\n",
       "      <td>5.374</td>\n",
       "      <td>0.498</td>\n",
       "      <td>233.290</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:27:00</td>\n",
       "      <td>5.388</td>\n",
       "      <td>0.502</td>\n",
       "      <td>233.740</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:28:00</td>\n",
       "      <td>3.666</td>\n",
       "      <td>0.528</td>\n",
       "      <td>235.680</td>\n",
       "      <td>15.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time Global_active_power Global_reactive_power  Voltage  \\\n",
       "0  16/12/2006  17:24:00               4.216                 0.418  234.840   \n",
       "1  16/12/2006  17:25:00               5.360                 0.436  233.630   \n",
       "2  16/12/2006  17:26:00               5.374                 0.498  233.290   \n",
       "3  16/12/2006  17:27:00               5.388                 0.502  233.740   \n",
       "4  16/12/2006  17:28:00               3.666                 0.528  235.680   \n",
       "\n",
       "  Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n",
       "0           18.400          0.000          1.000            17.0  \n",
       "1           23.000          0.000          1.000            16.0  \n",
       "2           23.000          0.000          2.000            17.0  \n",
       "3           23.000          0.000          1.000            17.0  \n",
       "4           15.800          0.000          1.000            17.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and perform initial preprocessing\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(data_file, sep=';', low_memory=False)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Date'].iloc[0]} to {df['Date'].iloc[-1]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb36ecc",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c714a89",
   "metadata": {},
   "source": [
    "### 1. Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e35b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean the household power consumption dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Raw dataset\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Cleaned dataset with datetime index\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Combine Date and Time columns\n",
    "    df_clean['DateTime'] = pd.to_datetime(df_clean['Date'] + ' ' + df_clean['Time'], \n",
    "                                         format='%d/%m/%Y %H:%M:%S')\n",
    "    \n",
    "    # Replace '?' with NaN and convert to numeric\n",
    "    numeric_columns = ['Global_active_power', 'Global_reactive_power', 'Voltage', \n",
    "                      'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    \n",
    "    # Set DateTime as index\n",
    "    df_clean.set_index('DateTime', inplace=True)\n",
    "    \n",
    "    # Drop original Date and Time columns\n",
    "    df_clean.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply cleaning\n",
    "df_clean = clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d78ab",
   "metadata": {},
   "source": [
    "### 1.1. Data Downsampling\n",
    "\n",
    "The original dataset has 1-minute resolution, which results in a very large dataset. For this tutorial, we'll downsample to 15-minute intervals to reduce computational complexity while preserving the essential patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0442d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_time_series(df, freq='15T', aggregation_method='mean'):\n",
    "    \"\"\"\n",
    "    Downsample time series data to a lower frequency.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe with datetime index\n",
    "    freq (str): Target frequency (e.g., '15T' for 15 minutes, '1H' for 1 hour)\n",
    "    aggregation_method (str): Method to aggregate values ('mean', 'sum', 'max', 'min')\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Downsampled dataframe\n",
    "    dict: Downsampling information\n",
    "    \"\"\"\n",
    "    original_freq = pd.infer_freq(df.index)\n",
    "    original_samples = len(df)\n",
    "    \n",
    "    # Apply downsampling based on aggregation method\n",
    "    if aggregation_method == 'mean':\n",
    "        df_downsampled = df.resample(freq).mean()\n",
    "    elif aggregation_method == 'sum':\n",
    "        df_downsampled = df.resample(freq).sum()\n",
    "    elif aggregation_method == 'max':\n",
    "        df_downsampled = df.resample(freq).max()\n",
    "    elif aggregation_method == 'min':\n",
    "        df_downsampled = df.resample(freq).min()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported aggregation method: {aggregation_method}\")\n",
    "    \n",
    "    # Remove rows with all NaN values (can occur at the boundaries)\n",
    "    df_downsampled = df_downsampled.dropna(how='all')\n",
    "    \n",
    "    downsampled_samples = len(df_downsampled)\n",
    "    reduction_factor = original_samples / downsampled_samples\n",
    "    \n",
    "    downsample_info = {\n",
    "        'original_frequency': original_freq,\n",
    "        'new_frequency': freq,\n",
    "        'original_samples': original_samples,\n",
    "        'downsampled_samples': downsampled_samples,\n",
    "        'reduction_factor': reduction_factor,\n",
    "        'aggregation_method': aggregation_method,\n",
    "        'data_reduction_percent': (1 - downsampled_samples / original_samples) * 100\n",
    "    }\n",
    "    \n",
    "    return df_downsampled, downsample_info\n",
    "\n",
    "print(\"Downsampling function implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply downsampling to 15-minute intervals\n",
    "print(\"Applying downsampling to 15-minute intervals...\")\n",
    "\n",
    "# Downsample the cleaned data\n",
    "df_downsampled, downsample_info = downsample_time_series(\n",
    "    df_clean, \n",
    "    freq='15T',  # 15 minutes\n",
    "    aggregation_method='mean'\n",
    ")\n",
    "\n",
    "# Print downsampling statistics\n",
    "print(\"=== DOWNSAMPLING SUMMARY ===\")\n",
    "print(f\"Original frequency: {downsample_info['original_frequency']}\")\n",
    "print(f\"New frequency: {downsample_info['new_frequency']}\")\n",
    "print(f\"Original samples: {downsample_info['original_samples']:,}\")\n",
    "print(f\"Downsampled samples: {downsample_info['downsampled_samples']:,}\")\n",
    "print(f\"Reduction factor: {downsample_info['reduction_factor']:.1f}x\")\n",
    "print(f\"Data reduction: {downsample_info['data_reduction_percent']:.1f}%\")\n",
    "print(f\"Aggregation method: {downsample_info['aggregation_method']}\")\n",
    "\n",
    "# Update our working dataset to use the downsampled version\n",
    "print(f\"\\nUpdating working dataset to use 15-minute intervals...\")\n",
    "print(f\"Original dataset shape: {df_clean.shape}\")\n",
    "print(f\"Downsampled dataset shape: {df_downsampled.shape}\")\n",
    "\n",
    "# Use downsampled data for the rest of the preprocessing pipeline\n",
    "df_clean = df_downsampled.copy()\n",
    "\n",
    "print(\"✅ Downsampling completed. Using 15-minute interval data for further processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1441bd2",
   "metadata": {},
   "source": [
    "### 2. Missing Value Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb411938",
   "metadata": {},
   "source": [
    "#### Missing Value Strategy\n",
    "\n",
    "Our approach leverages the strong seasonal patterns in household energy consumption data:\n",
    "\n",
    "1. **Identify missing intervals** - Find continuous periods of missing data (>1 minute)\n",
    "2. **Seasonal imputation** - Use previous years' data at the same time periods\n",
    "3. **Weekly fallback** - If yearly data unavailable, use previous week's pattern\n",
    "4. **Pattern preservation** - Maintains natural consumption patterns rather than simple interpolation\n",
    "\n",
    "This method is particularly effective for energy data because:\n",
    "- Households have consistent daily routines\n",
    "- Seasonal patterns (heating/cooling) are predictable\n",
    "- Weekly patterns (weekday vs weekend) are stable\n",
    "- Historical data provides realistic consumption values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ec121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(dataset):\n",
    "    \"\"\"\n",
    "    Fill missing values in time series data using seasonal patterns.\n",
    "    \n",
    "    This function identifies NaN intervals and fills them using:\n",
    "    1. Previous years' data at the same time period (preferred)\n",
    "    2. Previous week's data if yearly data is not available\n",
    "    \n",
    "    Parameters:\n",
    "    dataset (pd.DataFrame): Input dataframe with datetime index and missing values\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with filled missing values\n",
    "    \"\"\"\n",
    "    nan_intervals = []\n",
    "    during_interval = False\n",
    "    interval_start_index = None\n",
    "    for dt, val in dataset.Global_active_power.items():\n",
    "        if during_interval:\n",
    "            if not np.isnan(val):\n",
    "                timedelta = dt - interval_start_index\n",
    "                if (timedelta) > pd.Timedelta(1, 'm'):\n",
    "                    nan_intervals.append((interval_start_index, dt, timedelta))\n",
    "                during_interval = False\n",
    "        elif np.isnan(val):\n",
    "            interval_start_index = dt\n",
    "            during_interval = True\n",
    "\n",
    "    def get_previous_years(timestamp, index):\n",
    "        timestamp_list = []\n",
    "        year_count = 1\n",
    "        while (timestamp - pd.Timedelta(year_count * 365, 'd')) in index:\n",
    "            timestamp_list.append(\n",
    "                timestamp - pd.Timedelta(year_count * 365, 'd'))\n",
    "            year_count = year_count + 1\n",
    "        return timestamp_list\n",
    "\n",
    "    def get_previous_week(timestamp, index):\n",
    "        if (timestamp - pd.Timedelta(7, 'd')) in index:\n",
    "            return timestamp - pd.Timedelta(7, 'd')\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    nan_intervals = pd.DataFrame(nan_intervals, columns=[\n",
    "                                 'start', 'stop', 'duration'])\n",
    "\n",
    "    new_df = dataset.copy()\n",
    "    for start, stop, _ in nan_intervals.itertuples(index=False):\n",
    "        start_previous = get_previous_years(start, dataset.index)\n",
    "        if start_previous:\n",
    "            stop_previous = get_previous_years(stop, dataset.index)\n",
    "            values = np.array([dataset.loc[t1: t2, :]\n",
    "                              for t1, t2 in zip(start_previous, stop_previous)])\n",
    "            new_df.loc[start: stop, :] = np.mean(values, axis=0)\n",
    "        else:\n",
    "            start_previous = get_previous_week(start, dataset.index)\n",
    "            if start_previous:\n",
    "                stop_previous = get_previous_week(stop, dataset.index)\n",
    "                new_df.loc[start: stop,\n",
    "                           :] = dataset.loc[start_previous: stop_previous, :].values\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7f4acb",
   "metadata": {},
   "source": [
    "### 3. Data Normalization and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deabcd30",
   "metadata": {},
   "source": [
    "#### Normalization Methods\n",
    "\n",
    "Feature normalization is crucial for time series forecasting models to ensure all features contribute equally during training and to improve convergence. We implement two commonly used normalization techniques:\n",
    "\n",
    "**1. Standardization (Z-score normalization)**\n",
    "- **Formula**: `(x - mean) / standard_deviation`\n",
    "- **Result**: Mean = 0, Standard deviation = 1\n",
    "- **Best for**: Neural networks, algorithms sensitive to feature scale\n",
    "- **Preserves**: Outliers (maintains relative distances)\n",
    "- **Use when**: Data follows normal distribution, outliers are meaningful\n",
    "\n",
    "**2. Min-Max Scaling**\n",
    "- **Formula**: `(x - min) / (max - min)`\n",
    "- **Result**: Values scaled to [0, 1] range\n",
    "- **Best for**: When you need bounded values, comparing features with different units\n",
    "- **Preserves**: Relative relationships between observations\n",
    "- **Use when**: Data distribution is uniform, bounded output is required\n",
    "\n",
    "**Key Considerations:**\n",
    "- **Fit on training data**: Scaler parameters are computed only from training set\n",
    "- **Transform consistently**: Apply same parameters to validation/test sets\n",
    "- **Inverse transformation**: Essential for interpreting model predictions\n",
    "- **Feature selection**: Automatically excludes binary indicators and cyclical features already in proper ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9643db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(df, method='standardize', columns=None, scaler_params=None):\n",
    "    \"\"\"\n",
    "    Normalize features using standardization or min-max scaling.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    method (str): 'standardize' or 'minmax'\n",
    "    columns (list): Columns to normalize (None for all numeric)\n",
    "    scaler_params (dict): Pre-computed scaler parameters (for transform mode)\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Normalized dataframe\n",
    "    dict: Scaler parameters for inverse transformation\n",
    "    \"\"\"\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # Determine columns to normalize\n",
    "    if columns is None:\n",
    "        # Select only numeric columns\n",
    "        numeric_columns = df_norm.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        # Exclude datetime-related columns that shouldn't be normalized\n",
    "        exclude_patterns = ['year', '_sin', '_cos', 'is_', 'days_', 'holiday_cluster', 'holidays_in_week']\n",
    "        columns = [col for col in numeric_columns \n",
    "                  if not any(pattern in col.lower() for pattern in exclude_patterns)]\n",
    "    \n",
    "    # Initialize scaler parameters if not provided\n",
    "    if scaler_params is None:\n",
    "        scaler_params = {}\n",
    "        fit_mode = True\n",
    "    else:\n",
    "        fit_mode = False\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in df_norm.columns:\n",
    "            continue\n",
    "            \n",
    "        if method == 'standardize':\n",
    "            if fit_mode:\n",
    "                mean_val = df_norm[col].mean()\n",
    "                std_val = df_norm[col].std()\n",
    "                scaler_params[col] = {'mean': mean_val, 'std': std_val, 'method': 'standardize'}\n",
    "            else:\n",
    "                mean_val = scaler_params[col]['mean']\n",
    "                std_val = scaler_params[col]['std']\n",
    "            \n",
    "            # Z-score normalization: (x - mean) / std\n",
    "            if std_val > 0:\n",
    "                df_norm[col] = (df_norm[col] - mean_val) / std_val\n",
    "            else:\n",
    "                df_norm[col] = 0  # Handle constant columns\n",
    "            \n",
    "        elif method == 'minmax':\n",
    "            if fit_mode:\n",
    "                min_val = df_norm[col].min()\n",
    "                max_val = df_norm[col].max()\n",
    "                scaler_params[col] = {'min': min_val, 'max': max_val, 'method': 'minmax'}\n",
    "            else:\n",
    "                min_val = scaler_params[col]['min']\n",
    "                max_val = scaler_params[col]['max']\n",
    "            \n",
    "            # Min-max normalization: (x - min) / (max - min)\n",
    "            range_val = max_val - min_val\n",
    "            if range_val > 0:\n",
    "                df_norm[col] = (df_norm[col] - min_val) / range_val\n",
    "            else:\n",
    "                df_norm[col] = 0  # Handle constant columns\n",
    "    \n",
    "    return df_norm, scaler_params\n",
    "\n",
    "def inverse_normalize_features(df_norm, scaler_params):\n",
    "    \"\"\"\n",
    "    Inverse transform normalized features back to original scale.\n",
    "    \n",
    "    Parameters:\n",
    "    df_norm (pd.DataFrame): Normalized dataframe\n",
    "    scaler_params (dict): Scaler parameters from normalize_features\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Features in original scale\n",
    "    \"\"\"\n",
    "    df_orig = df_norm.copy()\n",
    "    \n",
    "    for col, params in scaler_params.items():\n",
    "        if col not in df_orig.columns:\n",
    "            continue\n",
    "            \n",
    "        method = params['method']\n",
    "        \n",
    "        if method == 'standardize':\n",
    "            # Inverse z-score: x * std + mean\n",
    "            df_orig[col] = df_orig[col] * params['std'] + params['mean']\n",
    "            \n",
    "        elif method == 'minmax':\n",
    "            # Inverse min-max: x * (max - min) + min\n",
    "            range_val = params['max'] - params['min']\n",
    "            df_orig[col] = df_orig[col] * range_val + params['min']\n",
    "    \n",
    "    return df_orig\n",
    "\n",
    "print(\"Normalization functions implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3984d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Demonstrate normalization methods\n",
    "print(\"Testing normalization methods...\")\n",
    "\n",
    "# Create a sample dataset for demonstration\n",
    "sample_data = df_filled['2007-01-01':'2007-01-07'].copy()\n",
    "\n",
    "# Test both normalization methods\n",
    "methods = ['standardize', 'minmax']\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\n--- {method.upper()} NORMALIZATION ---\")\n",
    "    \n",
    "    # Select specific columns for demonstration\n",
    "    target_columns = ['Global_active_power', 'Voltage', 'Global_intensity']\n",
    "    \n",
    "    # Apply normalization\n",
    "    normalized_data, scaler_params = normalize_features(\n",
    "        sample_data, \n",
    "        method=method, \n",
    "        columns=target_columns\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results[method] = {\n",
    "        'data': normalized_data,\n",
    "        'params': scaler_params\n",
    "    }\n",
    "    \n",
    "    # Show statistics for each normalized column\n",
    "    for col in target_columns:\n",
    "        if col in normalized_data.columns:\n",
    "            stats = normalized_data[col].describe()\n",
    "            print(f\"{col}:\")\n",
    "            print(f\"  Range: [{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
    "            print(f\"  Mean: {stats['mean']:.3f}, Std: {stats['std']:.3f}\")\n",
    "\n",
    "# Visualize normalization effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original data\n",
    "col_to_plot = 'Global_active_power'\n",
    "original_data = sample_data[col_to_plot].dropna()\n",
    "\n",
    "axes[0].hist(original_data, bins=30, alpha=0.7, color='blue')\n",
    "axes[0].set_title(f'Original {col_to_plot}')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Standardized (z-score)\n",
    "standardized = results['standardize']['data'][col_to_plot].dropna()\n",
    "axes[1].hist(standardized, bins=30, alpha=0.7, color='green')\n",
    "axes[1].set_title(f'Standardized (Z-score)')\n",
    "axes[1].set_xlabel('Normalized Value')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Min-Max [0, 1]\n",
    "minmax_01 = results['minmax']['data'][col_to_plot].dropna()\n",
    "axes[2].hist(minmax_01, bins=30, alpha=0.7, color='orange')\n",
    "axes[2].set_title(f'Min-Max [0, 1]')\n",
    "axes[2].set_xlabel('Normalized Value')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test inverse transformation\n",
    "print(\"\\n--- TESTING INVERSE TRANSFORMATION ---\")\n",
    "for method in methods:\n",
    "    print(f\"\\n{method} method:\")\n",
    "    normalized_data = results[method]['data']\n",
    "    scaler_params = results[method]['params']\n",
    "    \n",
    "    # Apply inverse transformation\n",
    "    restored_data = inverse_normalize_features(normalized_data, scaler_params)\n",
    "    \n",
    "    # Compare original vs restored\n",
    "    for col in target_columns:\n",
    "        if col in sample_data.columns:\n",
    "            original = sample_data[col].dropna()\n",
    "            restored = restored_data[col].dropna()\n",
    "            \n",
    "            # Calculate difference (should be very small due to floating point precision)\n",
    "            max_diff = abs(original - restored).max()\n",
    "            print(f\"  {col}: Max difference = {max_diff:.2e} (should be ~0)\")\n",
    "\n",
    "# Show transformation ranges\n",
    "print(f\"\\n--- FEATURE RANGES AFTER NORMALIZATION ---\")\n",
    "for method in methods:\n",
    "    print(f\"\\n{method.upper()} method:\")\n",
    "    normalized_sample = results[method]['data']\n",
    "    for col in target_columns:\n",
    "        if col in normalized_sample.columns:\n",
    "            min_val = normalized_sample[col].min()\n",
    "            max_val = normalized_sample[col].max()\n",
    "            mean_val = normalized_sample[col].mean()\n",
    "            std_val = normalized_sample[col].std()\n",
    "            print(f\"  {col:20s}: [{min_val:6.3f}, {max_val:6.3f}], mean={mean_val:6.3f}, std={std_val:6.3f}\")\n",
    "\n",
    "print(f\"\\nNormalization demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56437540",
   "metadata": {},
   "source": [
    "### 4. Dataset Splitting for Time Series\n",
    "\n",
    "Time series data requires special consideration when splitting into train/validation/test sets to avoid data leakage and maintain temporal order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fcdabb",
   "metadata": {},
   "source": [
    "#### Time Series Splitting Strategy\n",
    "\n",
    "**Why ratios-based splitting?**\n",
    "\n",
    "For time series data, we must preserve temporal order and avoid data leakage. The ratios method splits data chronologically:\n",
    "\n",
    "- **Training set**: First portion of the time series (e.g., 70%)\n",
    "- **Validation set**: Middle portion for model tuning (e.g., 15%) \n",
    "- **Test set**: Final portion for final evaluation (e.g., 15%)\n",
    "\n",
    "**Key principles:**\n",
    "- **Chronological order**: Earlier data for training, later data for testing\n",
    "- **No shuffling**: Maintains temporal dependencies\n",
    "- **No overlap**: Prevents future information leaking into past predictions\n",
    "- **Representative splits**: Each set should cover sufficient time periods to capture patterns\n",
    "\n",
    "**Common ratios:**\n",
    "- **70/15/15**: Standard split for large datasets\n",
    "- **80/10/10**: When you have abundant training data\n",
    "- **60/20/20**: When you need more validation/test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6b155",
   "metadata": {},
   "source": [
    "#### Important Considerations for Production Use\n",
    "\n",
    "**⚠️ Tutorial Simplification Notice**\n",
    "\n",
    "The splitting function above is simplified for tutorial purposes. In production time series forecasting, you must ensure that both validation and test sets contain **at least one full seasonality cycle** (e.g., one complete year for yearly patterns).\n",
    "\n",
    "**Why seasonality matters:**\n",
    "\n",
    "- **Pattern coverage**: Models need to see complete seasonal cycles to learn patterns properly\n",
    "- **Robust evaluation**: Validation/test performance should reflect behavior across all seasons\n",
    "- **Avoid bias**: Splitting mid-season can create artificially good/bad performance metrics\n",
    "- **Real-world simulation**: Test sets should represent the full range of conditions the model will encounter\n",
    "\n",
    "**Production recommendations:**\n",
    "\n",
    "1. **Identify dominant seasonality**: For household energy data, this is typically yearly (heating/cooling cycles)\n",
    "2. **Minimum validation period**: At least 1 full year (preferably 1.5-2 years for robust validation)\n",
    "3. **Minimum test period**: At least 1 full year for reliable performance assessment\n",
    "4. **Buffer periods**: Consider adding small gaps between train/val/test to simulate real deployment delays\n",
    "\n",
    "**Example for multi-year data:**\n",
    "```python\n",
    "# For 4 years of data with yearly seasonality:\n",
    "# Train: First 2 years (50%)\n",
    "# Validation: Year 3 (25%) \n",
    "# Test: Year 4 (25%)\n",
    "```\n",
    "\n",
    "This ensures each split captures complete seasonal patterns and provides more reliable model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca7fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time_series_by_ratios(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Split time series data chronologically using ratio-based approach.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe with datetime index\n",
    "    train_ratio (float): Proportion for training set (default: 0.7)\n",
    "    val_ratio (float): Proportion for validation set (default: 0.15)  \n",
    "    test_ratio (float): Proportion for test set (default: 0.15)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (train_df, val_df, test_df) chronologically split dataframes\n",
    "    dict: Split information with dates and sizes\n",
    "    \"\"\"\n",
    "    # Validate ratios sum to 1.0\n",
    "    total_ratio = train_ratio + val_ratio + test_ratio\n",
    "    if abs(total_ratio - 1.0) > 1e-6:\n",
    "        raise ValueError(f\"Ratios must sum to 1.0, got {total_ratio}\")\n",
    "    \n",
    "    # Ensure data is sorted by datetime index\n",
    "    df_sorted = df.sort_index()\n",
    "    total_samples = len(df_sorted)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    train_end_idx = int(total_samples * train_ratio)\n",
    "    val_end_idx = int(total_samples * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split the data chronologically\n",
    "    train_df = df_sorted.iloc[:train_end_idx].copy()\n",
    "    val_df = df_sorted.iloc[train_end_idx:val_end_idx].copy()\n",
    "    test_df = df_sorted.iloc[val_end_idx:].copy()\n",
    "    \n",
    "    # Create split information\n",
    "    split_info = {\n",
    "        'total_samples': total_samples,\n",
    "        'train_samples': len(train_df),\n",
    "        'val_samples': len(val_df), \n",
    "        'test_samples': len(test_df),\n",
    "        'train_period': (train_df.index.min(), train_df.index.max()),\n",
    "        'val_period': (val_df.index.min(), val_df.index.max()),\n",
    "        'test_period': (test_df.index.min(), test_df.index.max()),\n",
    "        'ratios_used': (train_ratio, val_ratio, test_ratio)\n",
    "    }\n",
    "    \n",
    "    return train_df, val_df, test_df, split_info\n",
    "\n",
    "def visualize_time_series_split(split_info, df=None):\n",
    "    \"\"\"\n",
    "    Visualize the time series split to verify chronological order.\n",
    "    \n",
    "    Parameters:\n",
    "    split_info (dict): Split information from split_time_series_by_ratios\n",
    "    df (pd.DataFrame): Optional original dataframe for timeline visualization\n",
    "    \"\"\"\n",
    "    print(\"=== TIME SERIES SPLIT SUMMARY ===\")\n",
    "    print(f\"Total samples: {split_info['total_samples']:,}\")\n",
    "    print(f\"Train samples: {split_info['train_samples']:,} ({split_info['train_samples']/split_info['total_samples']*100:.1f}%)\")\n",
    "    print(f\"Val samples:   {split_info['val_samples']:,} ({split_info['val_samples']/split_info['total_samples']*100:.1f}%)\")\n",
    "    print(f\"Test samples:  {split_info['test_samples']:,} ({split_info['test_samples']/split_info['total_samples']*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n=== TIME PERIODS ===\")\n",
    "    print(f\"Train period: {split_info['train_period'][0]} to {split_info['train_period'][1]}\")\n",
    "    print(f\"Val period:   {split_info['val_period'][0]} to {split_info['val_period'][1]}\")  \n",
    "    print(f\"Test period:  {split_info['test_period'][0]} to {split_info['test_period'][1]}\")\n",
    "    \n",
    "    # Calculate period durations\n",
    "    train_duration = split_info['train_period'][1] - split_info['train_period'][0]\n",
    "    val_duration = split_info['val_period'][1] - split_info['val_period'][0]\n",
    "    test_duration = split_info['test_period'][1] - split_info['test_period'][0]\n",
    "    \n",
    "    print(f\"\\n=== PERIOD DURATIONS ===\")\n",
    "    print(f\"Train duration: {train_duration}\")\n",
    "    print(f\"Val duration:   {val_duration}\")\n",
    "    print(f\"Test duration:  {test_duration}\")\n",
    "    \n",
    "    # Optional timeline visualization\n",
    "    if df is not None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        \n",
    "        # Plot a sample of the data to show timeline\n",
    "        if 'Global_active_power' in df.columns:\n",
    "            sample_data = df['Global_active_power'].dropna()\n",
    "            ax.plot(sample_data.index, sample_data.values, alpha=0.7, linewidth=0.5, color='gray')\n",
    "            \n",
    "            # Add vertical lines to show split boundaries\n",
    "            ax.axvline(split_info['train_period'][1], color='red', linestyle='--', \n",
    "                      label=f\"Train/Val split\", alpha=0.8)\n",
    "            ax.axvline(split_info['val_period'][1], color='orange', linestyle='--', \n",
    "                      label=f\"Val/Test split\", alpha=0.8)\n",
    "            \n",
    "            # Color the background regions\n",
    "            ax.axvspan(split_info['train_period'][0], split_info['train_period'][1], \n",
    "                      alpha=0.2, color='blue', label='Train')\n",
    "            ax.axvspan(split_info['val_period'][0], split_info['val_period'][1], \n",
    "                      alpha=0.2, color='green', label='Validation')\n",
    "            ax.axvspan(split_info['test_period'][0], split_info['test_period'][1], \n",
    "                      alpha=0.2, color='red', label='Test')\n",
    "            \n",
    "            ax.set_title('Time Series Split Visualization')\n",
    "            ax.set_xlabel('Date')\n",
    "            ax.set_ylabel('Global Active Power (kW)')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "print(\"Time series splitting functions implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Demonstrate time series splitting\n",
    "print(\"Demonstrating time series splitting...\")\n",
    "\n",
    "# Use the cleaned and filled dataset\n",
    "df_for_splitting = df_filled.copy()\n",
    "\n",
    "# Example 1: Standard 70/15/15 split\n",
    "print(\"\\n=== EXAMPLE 1: Standard 70/15/15 Split ===\")\n",
    "train_df, val_df, test_df, split_info = split_time_series_by_ratios(\n",
    "    df_for_splitting, \n",
    "    train_ratio=0.7, \n",
    "    val_ratio=0.15, \n",
    "    test_ratio=0.15\n",
    ")\n",
    "\n",
    "# Visualize the split\n",
    "visualize_time_series_split(split_info, df_for_splitting)\n",
    "\n",
    "# Example 2: Different ratios for comparison\n",
    "print(\"\\n\\n=== EXAMPLE 2: Alternative 80/10/10 Split ===\")\n",
    "train_df2, val_df2, test_df2, split_info2 = split_time_series_by_ratios(\n",
    "    df_for_splitting,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1, \n",
    "    test_ratio=0.1\n",
    ")\n",
    "\n",
    "# Show summary only (without plot to avoid clutter)\n",
    "print(\"=== TIME SERIES SPLIT SUMMARY ===\")\n",
    "print(f\"Total samples: {split_info2['total_samples']:,}\")\n",
    "print(f\"Train samples: {split_info2['train_samples']:,} ({split_info2['train_samples']/split_info2['total_samples']*100:.1f}%)\")\n",
    "print(f\"Val samples:   {split_info2['val_samples']:,} ({split_info2['val_samples']/split_info2['total_samples']*100:.1f}%)\")\n",
    "print(f\"Test samples:  {split_info2['test_samples']:,} ({split_info2['test_samples']/split_info2['total_samples']*100:.1f}%)\")\n",
    "\n",
    "# Show data integrity verification\n",
    "print(\"\\n=== DATA INTEGRITY VERIFICATION ===\")\n",
    "print(\"Checking chronological order...\")\n",
    "print(f\"Train end:   {train_df.index.max()}\")\n",
    "print(f\"Val start:   {val_df.index.min()}\")\n",
    "print(f\"Val end:     {val_df.index.max()}\")\n",
    "print(f\"Test start:  {test_df.index.min()}\")\n",
    "\n",
    "# Verify no overlap\n",
    "train_val_gap = val_df.index.min() - train_df.index.max()\n",
    "val_test_gap = test_df.index.min() - val_df.index.max()\n",
    "\n",
    "print(f\"\\nGaps between sets:\")\n",
    "print(f\"Train-Val gap:  {train_val_gap}\")\n",
    "print(f\"Val-Test gap:   {val_test_gap}\")\n",
    "\n",
    "if train_val_gap >= pd.Timedelta(0) and val_test_gap >= pd.Timedelta(0):\n",
    "    print(\"✅ Chronological order maintained - no data leakage!\")\n",
    "else:\n",
    "    print(\"❌ Warning: Potential data leakage detected!\")\n",
    "\n",
    "# Show sample data from each split\n",
    "print(f\"\\n=== SAMPLE DATA FROM EACH SPLIT ===\")\n",
    "print(\"Train set (first 3 rows):\")\n",
    "print(train_df[['Global_active_power', 'Voltage']].head(3))\n",
    "\n",
    "print(\"\\nValidation set (first 3 rows):\")\n",
    "print(val_df[['Global_active_power', 'Voltage']].head(3))\n",
    "\n",
    "print(\"\\nTest set (first 3 rows):\")\n",
    "print(test_df[['Global_active_power', 'Voltage']].head(3))\n",
    "\n",
    "print(f\"\\nTime series splitting demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d2c47",
   "metadata": {},
   "source": [
    "## Windowing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be56cdfc",
   "metadata": {},
   "source": [
    "### 1. Sliding Window Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(data, window_size, forecast_horizon=1, step_size=1):\n",
    "    \"\"\"\n",
    "    Create sliding windows for time series forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame or np.array): Time series data\n",
    "    window_size (int): Number of time steps in input window\n",
    "    forecast_horizon (int): Number of time steps to predict\n",
    "    step_size (int): Step size between windows\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X, y) arrays for model training\n",
    "    \"\"\"\n",
    "    # TODO: Implement sliding window creation\n",
    "    # - Create input sequences (X) and target sequences (y)\n",
    "    # - Handle multiple forecast horizons\n",
    "    # - Support different step sizes\n",
    "    # - Preserve temporal order\n",
    "    pass\n",
    "\n",
    "print(\"Sliding window function defined. Implementation needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c03b30",
   "metadata": {},
   "source": [
    "## Calendar Features Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1b7eb",
   "metadata": {},
   "source": [
    "### 1. Basic Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad784ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_features(df, datetime_col=None):\n",
    "    \"\"\"\n",
    "    Extract basic temporal features from datetime index or column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    datetime_col (str): Name of datetime column (None if using index)\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with added temporal features\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # Use datetime index or specified column\n",
    "    if datetime_col is None:\n",
    "        dt_series = df_temp.index\n",
    "    else:\n",
    "        dt_series = df_temp[datetime_col]\n",
    "    \n",
    "    # Basic time components\n",
    "    df_temp['hour'] = dt_series.hour\n",
    "    df_temp['day'] = dt_series.day\n",
    "    df_temp['month'] = dt_series.month\n",
    "    df_temp['year'] = dt_series.year\n",
    "    df_temp['dayofweek'] = dt_series.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df_temp['dayofyear'] = dt_series.dayofyear\n",
    "    df_temp['weekofyear'] = dt_series.isocalendar().week\n",
    "    df_temp['quarter'] = dt_series.quarter\n",
    "    \n",
    "    # Binary indicators\n",
    "    df_temp['is_weekend'] = (dt_series.dayofweek >= 5).astype(int)\n",
    "    df_temp['is_month_start'] = dt_series.is_month_start.astype(int)\n",
    "    df_temp['is_month_end'] = dt_series.is_month_end.astype(int)\n",
    "    df_temp['is_quarter_start'] = dt_series.is_quarter_start.astype(int)\n",
    "    df_temp['is_quarter_end'] = dt_series.is_quarter_end.astype(int)\n",
    "    \n",
    "    # Time-based categories\n",
    "    def get_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Winter'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Summer'\n",
    "        else:\n",
    "            return 'Autumn'\n",
    "    \n",
    "    df_temp['season'] = dt_series.month.map(get_season)\n",
    "    \n",
    "    # Hour categories for energy consumption patterns\n",
    "    def get_time_of_day(hour):\n",
    "        if 6 <= hour < 12:\n",
    "            return 'Morning'\n",
    "        elif 12 <= hour < 18:\n",
    "            return 'Afternoon'\n",
    "        elif 18 <= hour < 22:\n",
    "            return 'Evening'\n",
    "        else:\n",
    "            return 'Night'\n",
    "    \n",
    "    df_temp['time_of_day'] = dt_series.hour.map(get_time_of_day)\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "print(\"Temporal features function implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244f1db",
   "metadata": {},
   "source": [
    "### 2. Cyclical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f29c3",
   "metadata": {},
   "source": [
    "#### Understanding Fourier Features\n",
    "\n",
    "**What are Fourier Features?**\n",
    "\n",
    "Fourier features use trigonometric functions (sine and cosine) to represent periodic patterns in time series data. They are based on the mathematical principle that any periodic function can be decomposed into a sum of sine and cosine waves with different frequencies.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Harmonics** - Multiple frequencies that capture different aspects of periodicity\n",
    "   - k=1: Fundamental frequency (basic cycle)\n",
    "   - k=2: Second harmonic (captures sub-patterns within the cycle)\n",
    "   - k=3: Third harmonic (captures even finer periodic details)\n",
    "\n",
    "2. **Daily vs Yearly Cycles**\n",
    "   - **Daily**: 24-hour patterns (work/sleep, heating/cooling cycles)\n",
    "   - **Yearly**: Seasonal patterns (summer/winter energy usage)\n",
    "\n",
    "3. **Mathematical Representation**\n",
    "   ```\n",
    "   Daily features: sin(2πkt/day) and cos(2πkt/day)\n",
    "   Yearly features: sin(2πkt/year) and cos(2πkt/year)\n",
    "   ```\n",
    "\n",
    "**Advantages over Simple Cyclical Encoding:**\n",
    "\n",
    "- **Multiple harmonics** capture complex, non-sinusoidal patterns\n",
    "- **Better representation** of irregular but periodic behaviors  \n",
    "- **Proven effectiveness** in time series forecasting\n",
    "- **Flexible frequency modeling** with adjustable number of terms\n",
    "\n",
    "**Real-world Application:**\n",
    "For household energy consumption, Fourier features can capture:\n",
    "- Morning/evening usage peaks (daily patterns)\n",
    "- Weekday vs weekend differences (weekly sub-patterns)\n",
    "- Seasonal heating/cooling variations (yearly patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cyclical_features(df, columns):\n",
    "    \"\"\"\n",
    "    Create cyclical encoding for temporal features using sine and cosine.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    columns (dict): Dictionary mapping column names to their periods\n",
    "                   e.g., {'hour': 24, 'day_of_week': 7, 'month': 12}\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with cyclical features\n",
    "    \"\"\"\n",
    "    df_cyclic = df.copy()\n",
    "    \n",
    "    for column, period in columns.items():\n",
    "        if column in df_cyclic.columns:\n",
    "            # Create sine and cosine transformations\n",
    "            # Normalize to [0, 1] then convert to radians [0, 2π]\n",
    "            normalized = df_cyclic[column] / period\n",
    "            radians = 2 * np.pi * normalized\n",
    "            \n",
    "            # Add sine and cosine features\n",
    "            df_cyclic[f'{column}_sin'] = np.sin(radians)\n",
    "            df_cyclic[f'{column}_cos'] = np.cos(radians)\n",
    "    \n",
    "    return df_cyclic\n",
    "\n",
    "def create_fourier_features(dataset, num_fourier_terms=3):\n",
    "    \"\"\"\n",
    "    Create Fourier terms for daily and yearly cycles.\n",
    "    \n",
    "    Fourier features capture periodic patterns more effectively than simple\n",
    "    cyclical encoding by using multiple harmonics.\n",
    "    \n",
    "    Parameters:\n",
    "    dataset (pd.DataFrame): Input dataframe with datetime index\n",
    "    num_fourier_terms (int): Number of Fourier terms to generate\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with added Fourier features\n",
    "    list: List of column names that were added\n",
    "    \"\"\"\n",
    "    dataset = dataset.copy()\n",
    "    used_columns = []\n",
    "    \n",
    "    # Convert datetime index to timestamp in seconds\n",
    "    timestamp_s = dataset.index.astype('int64') // 10**9\n",
    "    \n",
    "    # Define time periods in seconds\n",
    "    day = 24*60*60\n",
    "    year = (365.2425)*day\n",
    "\n",
    "    for k in range(1, num_fourier_terms + 1):\n",
    "        # Daily Fourier terms\n",
    "        prefix = 'Day'\n",
    "        col_name = prefix + f\" sin{k}\"\n",
    "        dataset[col_name] = np.sin(timestamp_s * (2 * k * np.pi / day))\n",
    "        used_columns.append(col_name)\n",
    "\n",
    "        col_name = prefix + f\" cos{k}\"\n",
    "        dataset[col_name] = np.cos(timestamp_s * (2 * k * np.pi / day))\n",
    "        used_columns.append(col_name)\n",
    "\n",
    "        # Yearly Fourier terms\n",
    "        prefix = 'Year'\n",
    "        col_name = prefix + f\" sin{k}\"\n",
    "        dataset[col_name] = np.sin(timestamp_s * (2 * k * np.pi / year))\n",
    "        used_columns.append(col_name)\n",
    "\n",
    "        col_name = prefix + f\" cos{k}\"\n",
    "        dataset[col_name] = np.cos(timestamp_s * (2 * k * np.pi / year))\n",
    "        used_columns.append(col_name)\n",
    "    \n",
    "    return dataset, used_columns\n",
    "\n",
    "def visualize_cyclical_features(df, feature_name, period):\n",
    "    \"\"\"\n",
    "    Visualize cyclical features to verify encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe with cyclical features\n",
    "    feature_name (str): Name of the original feature\n",
    "    period (int): Period of the cyclical feature\n",
    "    \"\"\"\n",
    "    # TODO: Implement visualization\n",
    "    # - Plot original vs cyclical features\n",
    "    # - Show circular representation\n",
    "    # - Validate encoding correctness\n",
    "    pass\n",
    "\n",
    "print(\"Cyclical encoding functions defined. Implementation needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744703cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create and visualize Fourier features\n",
    "print(\"Creating Fourier features...\")\n",
    "\n",
    "# Apply missing value handling first\n",
    "df_filled = fill_missing_values(df_clean)\n",
    "\n",
    "# Create Fourier features\n",
    "df_fourier, fourier_columns = create_fourier_features(df_filled, num_fourier_terms=3)\n",
    "\n",
    "print(f\"Added {len(fourier_columns)} Fourier features:\")\n",
    "print(fourier_columns)\n",
    "\n",
    "# Plot the Fourier features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sample a week for visualization\n",
    "sample_start = '2007-01-01'\n",
    "sample_end = '2007-01-07'\n",
    "df_sample = df_fourier[sample_start:sample_end]\n",
    "\n",
    "# Plot daily Fourier terms\n",
    "axes[0, 0].plot(df_sample.index, df_sample['Day sin1'], label='Day sin1', alpha=0.7)\n",
    "axes[0, 0].plot(df_sample.index, df_sample['Day cos1'], label='Day cos1', alpha=0.7)\n",
    "axes[0, 0].set_title('Daily Fourier Terms (k=1)')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot yearly Fourier terms (zoomed to see variation)\n",
    "axes[0, 1].plot(df_sample.index, df_sample['Year sin1'], label='Year sin1', alpha=0.7)\n",
    "axes[0, 1].plot(df_sample.index, df_sample['Year cos1'], label='Year cos1', alpha=0.7)\n",
    "axes[0, 1].set_title('Yearly Fourier Terms (k=1)')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot multiple daily harmonics\n",
    "axes[1, 0].plot(df_sample.index, df_sample['Day sin1'], label='k=1', alpha=0.7)\n",
    "axes[1, 0].plot(df_sample.index, df_sample['Day sin2'], label='k=2', alpha=0.7)\n",
    "axes[1, 0].plot(df_sample.index, df_sample['Day sin3'], label='k=3', alpha=0.7)\n",
    "axes[1, 0].set_title('Daily Sine Terms (Multiple Harmonics)')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot correlation with original power data\n",
    "axes[1, 1].scatter(df_sample['Day sin1'], df_sample['Global_active_power'], \n",
    "                   alpha=0.6, s=20, label='Day sin1 vs Power')\n",
    "axes[1, 1].set_title('Fourier Feature vs Power Consumption')\n",
    "axes[1, 1].set_xlabel('Day sin1')\n",
    "axes[1, 1].set_ylabel('Global Active Power (kW)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlation analysis\n",
    "print(\"\\nCorrelation with Global Active Power:\")\n",
    "correlations = {}\n",
    "for col in fourier_columns:\n",
    "    corr = df_fourier[col].corr(df_fourier['Global_active_power'])\n",
    "    correlations[col] = corr\n",
    "    \n",
    "# Sort by absolute correlation\n",
    "sorted_corr = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "for col, corr in sorted_corr:\n",
    "    print(f\"{col:12s}: {corr:6.3f}\")\n",
    "\n",
    "print(f\"\\nDataset shape after adding Fourier features: {df_fourier.shape}\")\n",
    "print(f\"Original shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67700411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create and visualize cyclical features\n",
    "print(\"Creating temporal and cyclical features...\")\n",
    "\n",
    "# First extract temporal features\n",
    "df_temporal = extract_temporal_features(df_filled)\n",
    "\n",
    "# Define cyclical columns and their periods\n",
    "cyclical_columns = {\n",
    "    'hour': 24,           # 24 hours in a day\n",
    "    'dayofweek': 7,      # 7 days in a week\n",
    "    'month': 12,         # 12 months in a year\n",
    "    'dayofyear': 365     # 365 days in a year (approximately)\n",
    "}\n",
    "\n",
    "# Create cyclical features\n",
    "df_cyclical = create_cyclical_features(df_temporal, cyclical_columns)\n",
    "\n",
    "print(\"Added cyclical features:\")\n",
    "cyclical_feature_names = []\n",
    "for col in cyclical_columns.keys():\n",
    "    if col in df_temporal.columns:\n",
    "        cyclical_feature_names.extend([f'{col}_sin', f'{col}_cos'])\n",
    "\n",
    "print(cyclical_feature_names)\n",
    "\n",
    "# Visualize cyclical encoding\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sample data for visualization\n",
    "sample_data = df_cyclical['2007-01-01':'2007-01-07']\n",
    "\n",
    "# Plot 1: Hour cyclical encoding\n",
    "axes[0, 0].scatter(sample_data['hour'], sample_data['hour_sin'], alpha=0.6, s=20, label='Hour sin')\n",
    "axes[0, 0].scatter(sample_data['hour'], sample_data['hour_cos'], alpha=0.6, s=20, label='Hour cos')\n",
    "axes[0, 0].set_title('Cyclical Encoding: Hour')\n",
    "axes[0, 0].set_xlabel('Hour of Day')\n",
    "axes[0, 0].set_ylabel('Cyclical Value')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Day of week cyclical encoding\n",
    "axes[0, 1].scatter(sample_data['dayofweek'], sample_data['dayofweek_sin'], alpha=0.6, s=20, label='Day sin')\n",
    "axes[0, 1].scatter(sample_data['dayofweek'], sample_data['dayofweek_cos'], alpha=0.6, s=20, label='Day cos')\n",
    "axes[0, 1].set_title('Cyclical Encoding: Day of Week')\n",
    "axes[0, 1].set_xlabel('Day of Week (0=Mon, 6=Sun)')\n",
    "axes[0, 1].set_ylabel('Cyclical Value')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Circular representation of hour\n",
    "theta = np.linspace(0, 2*np.pi, 24)\n",
    "hours = np.arange(24)\n",
    "hour_sin = np.sin(theta)\n",
    "hour_cos = np.cos(theta)\n",
    "\n",
    "axes[1, 0].plot(hour_cos, hour_sin, 'b-', alpha=0.5, linewidth=2)\n",
    "axes[1, 0].scatter(hour_cos, hour_sin, c=hours, cmap='viridis', s=50)\n",
    "axes[1, 0].set_title('Circular Representation: 24 Hours')\n",
    "axes[1, 0].set_xlabel('Hour cos')\n",
    "axes[1, 0].set_ylabel('Hour sin')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_aspect('equal')\n",
    "\n",
    "# Add hour labels\n",
    "for i, (x, y) in enumerate(zip(hour_cos[::3], hour_sin[::3])):\n",
    "    axes[1, 0].annotate(f'{i*3}h', (x, y), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Plot 4: Cyclical vs Linear comparison for hour\n",
    "time_sample = sample_data.head(48)  # 2 days of hourly data\n",
    "axes[1, 1].plot(time_sample.index, time_sample['hour'], 'r-', label='Linear Hour', alpha=0.7)\n",
    "axes[1, 1].plot(time_sample.index, time_sample['hour_sin'], 'b-', label='Hour sin', alpha=0.7)\n",
    "axes[1, 1].plot(time_sample.index, time_sample['hour_cos'], 'g-', label='Hour cos', alpha=0.7)\n",
    "axes[1, 1].set_title('Linear vs Cyclical Hour Encoding')\n",
    "axes[1, 1].set_xlabel('Time')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset shape after adding temporal and cyclical features: {df_cyclical.shape}\")\n",
    "print(f\"Original shape: {df_clean.shape}\")\n",
    "\n",
    "# Show some sample cyclical features\n",
    "print(\"\\nSample cyclical features:\")\n",
    "print(df_cyclical[['hour', 'hour_sin', 'hour_cos', 'dayofweek', 'dayofweek_sin', 'dayofweek_cos']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183eba97",
   "metadata": {},
   "source": [
    "### 3. Holiday and Special Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_holiday_features(df, country='France'):\n",
    "    \"\"\"\n",
    "    Add holiday indicators and special events.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe with datetime index\n",
    "    country (str): Country for holiday calendar\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with holiday features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import holidays\n",
    "    except ImportError:\n",
    "        print(\"Warning: 'holidays' library not installed. Installing...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"holidays\"])\n",
    "        import holidays\n",
    "    \n",
    "    df_holidays = df.copy()\n",
    "    \n",
    "    # Get country-specific holidays for all years in the dataset\n",
    "    years = range(df.index.year.min(), df.index.year.max() + 1)\n",
    "    country_holidays = holidays.country_holidays(country, years=years)\n",
    "    \n",
    "    # Create date column for holiday lookup (without time component)\n",
    "    dates = df.index.date\n",
    "    \n",
    "    # Basic holiday indicator\n",
    "    df_holidays['is_holiday'] = [date in country_holidays for date in dates]\n",
    "    \n",
    "    # Holiday name (for analysis, can be dropped later)\n",
    "    df_holidays['holiday_name'] = [\n",
    "        country_holidays.get(date, '') for date in dates\n",
    "    ]\n",
    "    \n",
    "    # Days before/after holidays\n",
    "    for days_offset in [1, 2, 3]:\n",
    "        # Days before holiday\n",
    "        before_col = f'days_before_holiday_{days_offset}'\n",
    "        df_holidays[before_col] = 0\n",
    "        \n",
    "        # Days after holiday  \n",
    "        after_col = f'days_after_holiday_{days_offset}'\n",
    "        df_holidays[after_col] = 0\n",
    "        \n",
    "        for i, date in enumerate(dates):\n",
    "            # Check if future date is a holiday (days before)\n",
    "            future_date = date + pd.Timedelta(days=days_offset)\n",
    "            if future_date in country_holidays:\n",
    "                df_holidays.iloc[i, df_holidays.columns.get_loc(before_col)] = 1\n",
    "            \n",
    "            # Check if past date was a holiday (days after)\n",
    "            past_date = date - pd.Timedelta(days=days_offset)\n",
    "            if past_date in country_holidays:\n",
    "                df_holidays.iloc[i, df_holidays.columns.get_loc(after_col)] = 1\n",
    "    \n",
    "    # French-specific vacation periods (can be customized for other countries)\n",
    "    if country.lower() in ['france', 'fr']:\n",
    "        # Summer vacation (typically July-August)\n",
    "        df_holidays['is_summer_vacation'] = (\n",
    "            (df.index.month == 7) | (df.index.month == 8)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Winter vacation (around Christmas/New Year)\n",
    "        df_holidays['is_winter_vacation'] = (\n",
    "            ((df.index.month == 12) & (df.index.day >= 20)) |\n",
    "            ((df.index.month == 1) & (df.index.day <= 10))\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Spring vacation (typically around Easter, approximate)\n",
    "        df_holidays['is_spring_vacation'] = (\n",
    "            ((df.index.month == 4) & (df.index.day >= 10) & (df.index.day <= 25)) |\n",
    "            ((df.index.month == 3) & (df.index.day >= 25))\n",
    "        ).astype(int)\n",
    "    \n",
    "    # Holiday clustering (consecutive holiday days)\n",
    "    df_holidays['holiday_cluster_size'] = 0\n",
    "    is_holiday_series = df_holidays['is_holiday'].astype(bool)\n",
    "    \n",
    "    # Find consecutive holiday groups\n",
    "    holiday_groups = (is_holiday_series != is_holiday_series.shift()).cumsum()\n",
    "    for group_id in holiday_groups[is_holiday_series].unique():\n",
    "        group_mask = (holiday_groups == group_id) & is_holiday_series\n",
    "        cluster_size = group_mask.sum()\n",
    "        df_holidays.loc[group_mask, 'holiday_cluster_size'] = cluster_size\n",
    "    \n",
    "    # Holiday intensity (holidays within a week)\n",
    "    df_holidays['holidays_in_week'] = 0\n",
    "    for i in range(len(df_holidays)):\n",
    "        week_start = i - 3  # Look 3 days back\n",
    "        week_end = i + 4    # Look 3 days forward\n",
    "        week_start = max(0, week_start)\n",
    "        week_end = min(len(df_holidays), week_end)\n",
    "        \n",
    "        holidays_count = df_holidays['is_holiday'].iloc[week_start:week_end].sum()\n",
    "        df_holidays.iloc[i, df_holidays.columns.get_loc('holidays_in_week')] = holidays_count\n",
    "    \n",
    "    # Weekend-holiday interaction\n",
    "    df_holidays['is_weekend_holiday'] = (\n",
    "        df_holidays['is_holiday'] & \n",
    "        (df.index.dayofweek >= 5)  # Saturday or Sunday\n",
    "    ).astype(int)\n",
    "    \n",
    "    return df_holidays\n",
    "\n",
    "print(\"Holiday features function implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0512379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create and visualize holiday features\n",
    "print(\"Creating holiday features...\")\n",
    "\n",
    "# Apply holiday features to a subset of data for demonstration\n",
    "df_sample = df_filled['2007-01-01':'2007-12-31'].copy()  # Use one year for faster processing\n",
    "df_with_holidays = add_holiday_features(df_sample, country='France')\n",
    "\n",
    "print(f\"Added holiday features:\")\n",
    "holiday_columns = [col for col in df_with_holidays.columns if col not in df_sample.columns]\n",
    "print(holiday_columns)\n",
    "\n",
    "# Show holiday statistics\n",
    "print(f\"\\nHoliday Statistics for 2007:\")\n",
    "print(f\"Total holidays: {df_with_holidays['is_holiday'].sum()}\")\n",
    "print(f\"Weekend holidays: {df_with_holidays['is_weekend_holiday'].sum()}\")\n",
    "print(f\"Summer vacation days: {df_with_holidays['is_summer_vacation'].sum()}\")\n",
    "print(f\"Winter vacation days: {df_with_holidays['is_winter_vacation'].sum()}\")\n",
    "\n",
    "# Display some specific holidays\n",
    "holiday_dates = df_with_holidays[df_with_holidays['is_holiday'] == 1]\n",
    "print(f\"\\nSample holidays in 2007:\")\n",
    "print(holiday_dates[['holiday_name']].head(10))\n",
    "\n",
    "# Visualize holiday impact on energy consumption\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Holiday vs non-holiday consumption\n",
    "holiday_consumption = df_with_holidays[df_with_holidays['is_holiday'] == 1]['Global_active_power']\n",
    "normal_consumption = df_with_holidays[df_with_holidays['is_holiday'] == 0]['Global_active_power']\n",
    "\n",
    "axes[0, 0].hist(holiday_consumption.dropna(), bins=50, alpha=0.7, label='Holiday', density=True)\n",
    "axes[0, 0].hist(normal_consumption.dropna(), bins=50, alpha=0.7, label='Normal days', density=True)\n",
    "axes[0, 0].set_title('Energy Consumption: Holidays vs Normal Days')\n",
    "axes[0, 0].set_xlabel('Global Active Power (kW)')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Timeline showing holidays\n",
    "sample_month = df_with_holidays['2007-12-01':'2007-12-31']\n",
    "axes[0, 1].plot(sample_month.index, sample_month['Global_active_power'], 'b-', alpha=0.7, label='Power consumption')\n",
    "holiday_mask = sample_month['is_holiday'] == 1\n",
    "axes[0, 1].scatter(sample_month.index[holiday_mask], \n",
    "                   sample_month['Global_active_power'][holiday_mask], \n",
    "                   color='red', s=30, label='Holidays', zorder=5)\n",
    "axes[0, 1].set_title('December 2007: Energy Consumption with Holidays')\n",
    "axes[0, 1].set_ylabel('Global Active Power (kW)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Days before/after holiday effect\n",
    "days_before_1 = df_with_holidays[df_with_holidays['days_before_holiday_1'] == 1]['Global_active_power']\n",
    "days_after_1 = df_with_holidays[df_with_holidays['days_after_holiday_1'] == 1]['Global_active_power']\n",
    "\n",
    "box_data = [\n",
    "    normal_consumption.dropna().values,\n",
    "    holiday_consumption.dropna().values,\n",
    "    days_before_1.dropna().values,\n",
    "    days_after_1.dropna().values\n",
    "]\n",
    "box_labels = ['Normal', 'Holiday', 'Day Before', 'Day After']\n",
    "\n",
    "axes[1, 0].boxplot(box_data, labels=box_labels)\n",
    "axes[1, 0].set_title('Energy Consumption Around Holidays')\n",
    "axes[1, 0].set_ylabel('Global Active Power (kW)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Holiday cluster analysis\n",
    "cluster_sizes = df_with_holidays[df_with_holidays['holiday_cluster_size'] > 0]['holiday_cluster_size']\n",
    "cluster_consumption = []\n",
    "for size in range(1, cluster_sizes.max() + 1):\n",
    "    size_consumption = df_with_holidays[df_with_holidays['holiday_cluster_size'] == size]['Global_active_power']\n",
    "    if len(size_consumption) > 0:\n",
    "        cluster_consumption.append(size_consumption.mean())\n",
    "    else:\n",
    "        cluster_consumption.append(np.nan)\n",
    "\n",
    "valid_indices = ~np.isnan(cluster_consumption)\n",
    "axes[1, 1].bar(range(1, len(cluster_consumption) + 1), cluster_consumption, alpha=0.7)\n",
    "axes[1, 1].set_title('Average Consumption by Holiday Cluster Size')\n",
    "axes[1, 1].set_xlabel('Holiday Cluster Size (consecutive days)')\n",
    "axes[1, 1].set_ylabel('Average Global Active Power (kW)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "print(f\"\\nCorrelation with Global Active Power:\")\n",
    "holiday_features = ['is_holiday', 'days_before_holiday_1', 'days_after_holiday_1', \n",
    "                   'is_summer_vacation', 'is_winter_vacation', 'holidays_in_week']\n",
    "\n",
    "for feature in holiday_features:\n",
    "    if feature in df_with_holidays.columns:\n",
    "        corr = df_with_holidays[feature].corr(df_with_holidays['Global_active_power'])\n",
    "        print(f\"{feature:20s}: {corr:6.3f}\")\n",
    "\n",
    "print(f\"\\nDataset shape after adding holiday features: {df_with_holidays.shape}\")\n",
    "print(f\"Original sample shape: {df_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dfa8b0",
   "metadata": {},
   "source": [
    "## Lag Features and Rolling Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ede3e",
   "metadata": {},
   "source": [
    "### 1. Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df, columns, lags):\n",
    "    \"\"\"\n",
    "    Create lag features for specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    columns (list): Columns to create lags for\n",
    "    lags (list): List of lag periods\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with lag features\n",
    "    \"\"\"\n",
    "    # TODO: Implement lag feature creation\n",
    "    # - Create lagged versions of specified columns\n",
    "    # - Handle multiple lag periods\n",
    "    # - Manage missing values from lagging\n",
    "    pass\n",
    "\n",
    "print(\"Lag features function defined. Implementation needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd816689",
   "metadata": {},
   "source": [
    "### 2. Rolling Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rolling_features(df, columns, windows, statistics=['mean', 'std', 'min', 'max']):\n",
    "    \"\"\"\n",
    "    Create rolling window statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    columns (list): Columns to calculate statistics for\n",
    "    windows (list): List of window sizes\n",
    "    statistics (list): List of statistics to calculate\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with rolling features\n",
    "    \"\"\"\n",
    "    # TODO: Implement rolling statistics\n",
    "    # - Calculate rolling mean, std, min, max\n",
    "    # - Support multiple window sizes\n",
    "    # - Add rolling quantiles\n",
    "    # - Handle edge cases\n",
    "    pass\n",
    "\n",
    "def create_expanding_features(df, columns, statistics=['mean', 'std']):\n",
    "    \"\"\"\n",
    "    Create expanding window statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    columns (list): Columns to calculate statistics for\n",
    "    statistics (list): List of statistics to calculate\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with expanding features\n",
    "    \"\"\"\n",
    "    # TODO: Implement expanding statistics\n",
    "    # - Calculate expanding mean, std\n",
    "    # - Useful for trend analysis\n",
    "    pass\n",
    "\n",
    "print(\"Rolling statistics functions defined. Implementation needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ddcf5b",
   "metadata": {},
   "source": [
    "## Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c6da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesPreprocessor:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for time series forecasting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=24, forecast_horizon=1, \n",
    "                 normalization='standardize', add_cyclical=True):\n",
    "        \"\"\"\n",
    "        Initialize preprocessing pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        window_size (int): Input sequence length\n",
    "        forecast_horizon (int): Number of steps to predict\n",
    "        normalization (str): Normalization method\n",
    "        add_cyclical (bool): Whether to add cyclical features\n",
    "        \"\"\"\n",
    "        # TODO: Initialize pipeline components\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df):\n",
    "        \"\"\"\n",
    "        Fit preprocessing pipeline on training data.\n",
    "        \n",
    "        Parameters:\n",
    "        df (pd.DataFrame): Training dataset\n",
    "        \n",
    "        Returns:\n",
    "        self: Fitted preprocessor\n",
    "        \"\"\"\n",
    "        # TODO: Fit all preprocessing steps\n",
    "        pass\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Transform dataset using fitted pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        df (pd.DataFrame): Input dataset\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (X, y) for model training/prediction\n",
    "        \"\"\"\n",
    "        # TODO: Apply all preprocessing steps\n",
    "        pass\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"\n",
    "        Fit and transform in one step.\n",
    "        \n",
    "        Parameters:\n",
    "        df (pd.DataFrame): Input dataset\n",
    "        \n",
    "        Returns:\n",
    "        tuple: (X, y) for model training\n",
    "        \"\"\"\n",
    "        return self.fit(df).transform(df)\n",
    "    \n",
    "    def inverse_transform(self, predictions):\n",
    "        \"\"\"\n",
    "        Inverse transform predictions to original scale.\n",
    "        \n",
    "        Parameters:\n",
    "        predictions (np.array): Model predictions\n",
    "        \n",
    "        Returns:\n",
    "        np.array: Predictions in original scale\n",
    "        \"\"\"\n",
    "        # TODO: Implement inverse transformation\n",
    "        pass\n",
    "\n",
    "print(\"TimeSeriesPreprocessor class defined. Implementation needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6954bf15",
   "metadata": {},
   "source": [
    "## Usage Examples and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd2f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add example usage of the preprocessing pipeline\n",
    "# Example:\n",
    "# preprocessor = TimeSeriesPreprocessor(window_size=24, forecast_horizon=1)\n",
    "# X_train, y_train = preprocessor.fit_transform(df_train)\n",
    "# X_test, y_test = preprocessor.transform(df_test)\n",
    "\n",
    "print(\"Usage examples to be implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559e4e62",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive framework for time series preprocessing including:\n",
    "\n",
    "### Key Components:\n",
    "1. **Data Cleaning**: Handling missing values, data type conversion, datetime processing\n",
    "2. **Normalization**: Multiple scaling methods for different use cases\n",
    "3. **Windowing**: Sliding windows, overlapping sequences, multi-horizon support\n",
    "4. **Calendar Features**: Temporal features, cyclical encoding, holiday indicators\n",
    "5. **Lag Features**: Historical values and rolling statistics\n",
    "6. **Pipeline**: Complete preprocessing class for reproducible workflows\n",
    "\n",
    "### Next Steps:\n",
    "- Implement the defined functions with specific logic\n",
    "- Test with the household power consumption dataset\n",
    "- Validate preprocessing quality\n",
    "- Integrate with machine learning models\n",
    "\n",
    "### Best Practices:\n",
    "- Always preserve temporal order in time series data\n",
    "- Avoid data leakage when creating features\n",
    "- Use separate train/validation/test splits for time series\n",
    "- Validate all transformations are invertible\n",
    "- Document all preprocessing steps for reproducibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iecon2025-tutorial-z-J6dBct-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
